{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Auftrag\n",
    "\n",
    "Die Aufgabe besteht darin eine Web-Kunstgallerie mit Portraitfotos aller Studierenden zu erstellen.\n",
    "\n",
    "Jeder Studierende erstellt von sich ein Portraitfoto (Wer kein eigenes Foto verwenden will, der kann hier ein Portraitfoto erstellen).\n",
    "Mittels Neural Style Transfer (Jupyter Notebook) soll davon ein Portrait im Stil eines beliebigen Künstlers (van Gogh, Da Vinci, Picasso, Dürer, etc.) erstellt werden.\n",
    "Erstellt eine Gallerie und veröffentlicht es auf einer eigenen Webseite.\n",
    "\n",
    "Inspiriert von: \n",
    "* https://blog.paperspace.com/neural-style-transfer/\n",
    "* https://blog.tensorflow.org/2018/08/neural-style-transfer-creating-art-with-deep-learning.html"
   ],
   "id": "4672371061adc73a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parameter definieren\n",
    "\n",
    "### Hyperparameter\n",
    "total_variation_weight:\n",
    "* Steuert die Gewichtung der Total Variation (TV)-Regularisierung. Total Variation ist eine Technik, um das Bild glatt zu machen und Rauschen zu reduzieren. Ein sehr kleiner Wert (wie 1e-6) lässt das Netzwerk mehr Freiheit, die Struktur und Details des Inhalts zu erhalten, während es gleichzeitig das Rauschen im Bild vermeidet.\n",
    "\n",
    "style_weight:\n",
    "* Gewichtet den Stilverlust (der Unterschied zwischen den Stilmerkmalen des generierten Bildes und des Stilbildes). Ein kleiner Wert wie 1e-6 bedeutet, dass der Stil im Vergleich zum Inhalt nur leicht gewichtet wird. Das ist typisch, wenn der Fokus auf dem Erhalt des Inhalts liegt, und weniger auf der genauen Reproduktion des künstlerischen Stils.\n",
    "\n",
    "content_weight:\n",
    "* Bestimmt, wie stark der Inhalt des Bildes beibehalten wird. Ein kleiner Wert wie 2.5e-8 weist darauf hin, dass der Inhalt weniger stark berücksichtigt wird, was die kreative Freiheit des Stils stärkt.\n"
   ],
   "id": "7784af11ca740cf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_image_path = \"./data/input/not_jeremy.png\"\n",
    "style_reference_image_path = \"./data/styles/picasso_self_portrait.png\"\n",
    "result_prefix = \"not_jeremy\"\n",
    "\n",
    "# Hyperparameter\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ],
   "id": "39296d5689b5cd4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hilfsfunktionen",
   "id": "817dab388cdc6ebf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing des Bildes, Bild in ein Tensor kompatibles Format transferieren",
   "id": "1e88a0ef7c76a61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n"
   ],
   "id": "7d404e12869e59c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Funktion zum wieder erstellen des pre-preprocessed Bildes",
   "id": "1561a43ac93616a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ],
   "id": "39914fe73f8e9bd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Verlustfunktionen definieren\n",
    "\n",
    "Style loss: \n",
    "* keeps the generated image close to the local textures of the style reference image\n",
    "\n",
    "Content loss: \n",
    "* keeps the high-level representation of the generated image close to that of the base image"
   ],
   "id": "11364ed680062825"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gram_matrix(x):\n",
    "    # The gram matrix of an image tensor (feature-wise outer product)\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :])\n",
    "    b = tf.square(x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :])\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ],
   "id": "665d62a8fe840f95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model definieren",
   "id": "a2dfc0cdfa903d67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss"
   ],
   "id": "c0322cdcc771050c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bild generieren",
   "id": "42e13ad46daf8d6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "# iterations = 2000\n",
    "iterations = 5\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "num_rows = (iterations / 100) // 5\n",
    "start_time = time.time()\n",
    "global_start = time.time()\n",
    "\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)\n",
    "\n",
    "    if i > num_rows * 5: continue\n",
    "    plt.subplot(num_rows, 5, i)\n",
    "    plot_img = base_image.numpy()\n",
    "    plot_img = deprocess_image(plot_img)\n",
    "    plt.imshow(plot_img)\n",
    "    plt.title('Iteration {}'.format(i + 1))"
   ],
   "id": "fb0e1c59f6c1b7bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
